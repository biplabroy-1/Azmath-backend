print('hello from azmath files')

import faiss
import os
import numpy as np
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from sentence_transformers import SentenceTransformer


def search_files(file_path:str,query:str):
    
    
    os.environ["SERPAPI_API_KEY"] = "c03124527bf03e21aae7b483106d31f39b02029e627e1288906b623061471125"
    API = "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"
    llm = ChatGroq(temperature=0, model_name="mixtral-8x7b-32768", groq_api_key=API)
    
    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    # load text in a file path, Save Text in a file path
    def load_text_file(file_path: str) -> str:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
            file.read()

    
    # Vector search In chosen File
    def search_file(file_path: str, query_embedding: np.ndarray) -> str:
        if not os.path.exists(file_path):
            print(f"File not found: {file_path}")
            return None

        print(f"Loading and searching in file: {file_path}")
        text_data = load_text_file(file_path)

        if text_data is None:
            print(f"Failed to load text data from file: {file_path}")
            return None

        text_embeddings = embedding_model.encode([text_data])

        if text_embeddings is None:
            print(f"Failed to encode text data for file: {file_path}")
            return None

        try:
            index = faiss.IndexFlatL2(text_embeddings.shape[1])
            index.add(text_embeddings)

            D, I = index.search(query_embedding.reshape(1, -1).astype(np.float32), 1)

            if I[0][0] == -1:
                return None

            return text_data

        except Exception as e:
            print(f"Error during search_file for file {file_path}: {e}")
            return None

    def search_documents(file_path,query: str) -> str:
    # Convert query to embedding
        query_embedding = embedding_model.encode([query])
        print("Query Converted to Embedding")
        results = []

        result = search_file(file_path, query_embedding)
        results.append(result)

        if results:
            # Use LLM to generate a summary of the retrieved documents
            user_input = f"""
            Given the following query:
            {query}

            And the following retrieved documents:
            {results}

            Please generate a summary based on the context provided by the documents.
            """

            chat = llm
            prompt = ChatPromptTemplate.from_messages(
                [("system", "You are an intelligent assistant. You are supposed to answer any question the human asks."),
                ("human", "{input}")])

            chain = prompt | chat
            result = chain.invoke({"input": user_input})

            return str("generated By search in Local Knoledge Base : " + result.content)

        # If no results found, fallback to backup search
        backup_result = search(query)
        if backup_result:
            return backup_result
        else:
            # If not found in backup, return raw LLM output
            chat = llm
            prompt = ChatPromptTemplate.from_messages(
                [("system", "You are an intelligent assistant. You are supposed to answer any question the human asks."),
                ("human", "{input}")])

            chain = prompt | chat
            result = chain.invoke({"input": query})
            return result.content
            
        
    result = search_documents(file_path,query)
    print('bye from azmath files')
    return result